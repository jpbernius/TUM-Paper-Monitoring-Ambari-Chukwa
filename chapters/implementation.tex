\subsection{How is the concept of monitoring implemented in Ambari and Chukwa?}
\label{subsec:Implementation}

\subsubsection{Ambari}
\amb is an open source framework that allows the User to monitor, manage and install Hadoop.\cite{ChukwaPoster} It has the ability to start, stop and configure Hadoop processes automatically on clusters, without any intervention of the user.\cite{Hortonworks2013} \amb was founded to simplify the use of Hadoop.\cite{Hortonworks2013}
\\
\amb architecture can be divided in connected parts. First the \amb Web which is the main platform for Users to log in and give up request that should be run through \amb.\cite{Sako} It also visualize all the monitoring processes. Secondly the \amb Server, which is divided into smaller parts itself.\cite{Sako} The API or REST API is connected to different Web applications, the most important one of is \amb Web.\cite{Sako} Other interfaces like Microsoft System Centre, are applications that allows the user to analyse the data furthermore the capability of Hadoop.\cite{Hortonworks2013} The results of the analyse is accessible through \amb Web, on the monitoring screens.\cite{Sako}
\\
The connection between \amb and Hadoop is done by the \amb Agents.\cite{Sako} \amb Server installs on each hast an \amb Agent, that gives every few seconds a heartbeat to the server; the server answers with an instruction for the Agent or it sends a confirmation about his current life-status.\cite{Sako} All hosts are connected to the Clusters by Hadoop.\cite{Sako}
\\
\begin{figure}[hbt]
  \centering
  % https://docs.google.com/open?id=1A3TbXPEQeknqWxxX811XHG7pcQOuMEFDmo_hl8LlfYk
  \includegraphics[width=\linewidth,clip=true,trim=0 3cm 0 0]{images/AmbariArchitecture}
  \caption{Ambaris Architecture~\cite{Sako}}
  \label{fig:AmbariArchitecture}
\end{figure}

The concept of monitoring is given in two specific ways with \amb\. \amb has the knowledge about any cluster health and its data.\cite{Foley2012} It can provide and improve the health situation of a Hadoop cluster and analyse the data in ways there are useful for the user.\cite{Foley2012} The user has an overview on the current situation through the Web application.\cite{Foley2012} \amb divides to monitoring Process in three parts. \cite{Foley2012}The first one is the List of all the Application \amb is running on each connected clusters. It shows the name, status and number of clusters that are in default, and need to get fixed.\cite{Foley2012} The second part is explain all the technical details about the application, such as storing space and running time.\cite{Foley2012} The last part is aimed to show illustratively how long and effective an application has been running on any clusters. \cite{Foley2012}Based on this data the user can give up requests to change the applications or add more clusters into one process, through the API. \cite{Sako}\amb then will install automatically an new application to the cluster on sending heartbeat information to the \amb Agent.\cite{Sako} \amb has the ability of installing and giving an breve overlook about the health of the system; its not capable of analysing the data qualitatively.\cite{Sako}

\subsubsection{Chukwa}
\chuklong is ``a data collection system for monitoring and analysing large distributed systems.''~\cite{Boulona}
It is built to support log handling on scalable Hadoop clusters. \chuk is developed at Yahoo! and ``is built on top of HDFS and Map-Reduce.''~\cite{Rabkin2008a}
Its main non-functional goals are low footprint on System Usage (less than 5\%) and minimal changes to user workflow.~\cite{Rabkin2010}

\paragraph{The process}
For processing Log files and Application Metrics, \chuk starts on the producing machine, which is running a \chuk Agent. 
This piece of Java Software is constantly gathering Application Logs and Metrics from pre-configured locations. 
This can by \textit{syslog} as well as an Application specific location. 
Each Source is fetched by an so called \textit{Adaptor}.~\cite{ChukwaAdminAgent}

\begin{figure}[hbt]
  \centering
  % https://docs.google.com/open?id=11xCUWZ94zbUa4eqhoScjMmfIL2pI-IkDx02qvcCXohY
  \includegraphics[width=\linewidth,clip=true,trim=0 6cm 0 0]{images/ChukwaArchitecture}
  \caption{Chukwas Architecture~\cite{Rabkin2008}}
  \label{fig:ChukwaArchitecture}
\end{figure}

The Agent is then pushing the data to a so called \textit{Collector}, as described on Figure~\ref{fig:ChukwaArchitecture}.
This is another piece of Java Software, running on a dedicated \chuk instance. 
The \textit{Collector}s job is collecting data from multiple Nodes in the Cluster, as well as archiving and post-processing the data.~\cite{Jose2014}
First, the collected information is stored in \textit{Data Sink} Files. 
Here, data from multiple Nodes gets written in the same file, which can be seen as a Queue for processing.
Every few minutes, the \textit{Data Sink} File is closed. (Files cannot be processed, while data is written to them.)
All available \textit{Data Sink} files will be processed by a pair of Demux \mr jobs.~\cite{Boulona} 


As desribed by Boulon et al., the \mr processing should produce insights about ``needs for accounting, capacity planning, performance characterization, utilization'', ``reduc[e] the number and extent of outages'', ``reduc[e] the number of false alerts and increasing the value and confidence level of true alerts'' and ``reduc[e] the time and effort required to identify and resolve cluster issues.''\cite{Boulonb}

To achieve these goals, ``the first job simply archives all the collected data, without processing or interpreting it. The second job parses out structured data from some of the logs, and loads this structured data into a data store.''~\cite{Boulona} 

% reduce can be customised. https://chukwa.apache.org/docs/r0.4.0/programming.html

\paragraph{The Storage}

The \mr jobs will \demux data out of the \textit{sink} files and into structured storage.~\cite{Boulonb} 
\chuk supports multiple, pluggable storage formats, which ``also aids integration with legacy systems. Chukwa also offers the flexibility to support other data sources, such as syslog or local IPC.''~\cite{Rabkin2010a} 

Currently, \chuk uses \hdfs files, ``one file per cluster, per data type, and time period. (..) [Which is only a] interim solution''~\cite{Boulona} 
The development team is evaluating different data-stores, which are more suitable for the purpose and allow structured querying, also.
