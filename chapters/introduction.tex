%TODO: Introduction
As internet applications are increasing, more and more servers are getting integrated into distributed systems.~\cite{Dinu2011} Applications need to be run on multiple servers in parallel to fulfil their responsibilities. 
Further, the number of running nodes will vary based on user number and activity.~\cite{Jammes2012} 
As these systems get impossible to administer by hand, automating the process is very important.~\cite{Jammes2012} 
Systems tend to fail from time to time, therefore it is very useful to notice failure and fix occurring problems in advance. 
``This requires continuous monitoring along with online analysis of the data captured by the monitoring system.''~\cite{Kutare2010}
Jerome Boulon, Principle Engineer at Yahoo! and PPMC in the Chukwa team, stated that ``Large distributed systems can fail in complicated and subtle ways.''~\cite{Boulonb} 
Monitoring allows to notice and archive failures. 
The resulting data provides Developers and System Administrators with details about occurring failures and errors. 
The following paper will describe and compare two monitoring Systems, \amb and \chuk. 
Both systems use \hadoop and the \mr method as basic component.\cite{ApacheSoftwareFoundation2015}


\subsection{Definitions}

\textit{Monitoring} is a process, which controls, interacts and manages other processes and systems. 
It has the ability to locate, prevent and repair failures in advance. 
It is an important strategy for availability and security of procedures. 
Monitoring is always running parallel to activities and gives the user the ability to repair failures faster, then humans could possibly do manually.\cite{Jammes2012}
Ford said in 1990 that it is ``an important long-term goal (..) [to incorporate] such monitoring functionality in the highly distributed, ``inteligent'' next generation of [distributed] systems.''~\cite{Ford1990}

\amb is an open source framework that allows to control, manage, provide and install Hadoop Cluster easily from the included Web-Interface. It simplifies the use of Hadoop.\cite{Hortonworks2013}

\chuk is a large-scale Log Collection and Management System. It is specialised to collect Log files as well as Application Metrics from distributed Clusters.

\subsection{Structure of this paper}
First, the concepts of \hadoop and \mr~(\ref{subsec:Hadoop}) are explained, these concepts are the basis of the techniques introduced in this paper. 
Second, the concept of monitoring~(\ref{subsec:ConceptMonitoring}) is introduced and two interpretations and implementations, Apache Systems \amb and \chuk are explained~(\ref{subsec:Implementation}) in more detail. 
Third, a comparison~(\ref{subsec:Differentiation}) is made and, lastly, a conclusion~(\ref{sec:Conclusion}) is drawn.

\subsection{Motivation}
This paper has been written for the seminar \isdslong at the \tum. Main Goals are studying ``requirements and properties that distinguish Internet-scale distributed systems from other types of distributed systems'' and further how to ``satisfy these requirements at Internet-scale.''~\cite{SeminarInfo} 
When introducing systems, which scale to a couple of thousand nodes, it is not only important how the system works, but also how to get such a cluster up and running, check on the clusters health and get more detailed insights of possible failures.

\subsection{Literature Review}
As it is highly recommended to document the research and review process,~\cite{brocke09} this paragraph will summarise the research and review process.

The research started on the self description on the project websites and continued using technical documentation, describing the underlying architecture of both systems.
Further, Library Databases, especially \emph{IEEE Xplore} and \emph{EBSCOhost} were utilised to find more in-depth material, including the research these projects are based on. A lot of details about the use and monitoring process of \amb were found at Hortonworks.
	
\subsection{Hadoop}
\label{subsec:Hadoop}
\hadoopshort is an open source software by Apache for storing and analysing data.\cite{Dagli2014} It divides and storages Big Data across multiple smaller clusters, by copying them three times with the map and reduce function. This concept implies a high fault tolerance if one of the clusters fails.\cite{Dagli2014}

Hadoops Architecture is divided in many parts. To understand \amb and \chuk, the three main parts will be enough.\cite{Dagli2014}
  \begin{enumerate}
  	\item \hadoop Clusters are multiple new storage places for the data. They are divided into one master node and several server nodes, that are fulfilling the tasks of the master node.\cite{Dagli2014}
  	\item \hdfs, are file systems that are responsible for storing data across multiple machines.\cite{Dagli2014}
  	\item Map and Reduce is the main function which maps and reduces data until the core mean on all Hadoop clusters.\cite{Dagli2014}
  \end{enumerate}

\paragraph{Map Reduce}
The \mrlong process is the main function of \hadoop. After the division of data across multiple clusters, it maps similar datasets together into one cluster. 
Later on it analyses and reduces the data to the core. This data is getting combined together into the output. \cite{Dagli2014}

\hadoop Users need to configure the process for their data and use case. All the other parts of the function such as sort and store are already available when \hadoop is getting installed.\cite{Dagli2014}


\amb and \chuk both presuppose a basic understanding of \hadoop as well as the \mr function. Both applications are powered by Apache and are monitoring processes for \hadoop Systems.They differ in their area of operation of monitoring.\cite{ApacheSoftwareFoundation2015}

\paragraph{Demux}
The \demux process includes ``pars[ing and]  convert[ing]  unstructured  chunks  to  fully  or  semi  structured [Records].''~\cite{Boulon}